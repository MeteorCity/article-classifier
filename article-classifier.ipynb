{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load up all the documents in the corpus\n",
    "train_corpus = sc.textFile(\"s3://chrisjermainebucket/comp330_A5/TestingDataOneLinePerDoc.txt\")\n",
    "test_corpus = sc.textFile(\"s3://chrisjermainebucket/comp330_A5/SmallTrainingDataOneLinePerDoc.txt\")\n",
    "\n",
    "\n",
    "# each entry in validLines will be a line from the text file\n",
    "train_validLines = train_corpus.filter(lambda x : 'id' in x)\n",
    "test_validLines = test_corpus.filter(lambda x : 'id' in x)\n",
    "\n",
    "\n",
    "# now we transform it into a bunch of (docID, text) pairs\n",
    "train_keyAndText = train_validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:]))\n",
    "test_keyAndText = test_validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:]))\n",
    "\n",
    "\n",
    "# now we split the text in each (docID, text) pair into a list of words\n",
    "# after this, we have a data set with (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# we have a bit of fancy regular expression stuff here to make sure that we do not\n",
    "# die on some of the documents\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "train_keyAndListOfWords = train_keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "test_keyAndListOfWords = test_keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "\n",
    "\n",
    "# now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "train_allWords = train_keyAndListOfWords.flatMap(lambda x: ((j, 1) for j in x[1]))\n",
    "test_allWords = test_keyAndListOfWords.flatMap(lambda x: ((j, 1) for j in x[1]))\n",
    "\n",
    "\n",
    "# now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "train_allCounts = train_allWords.reduceByKey (lambda a, b: a + b)\n",
    "test_allCounts = test_allWords.reduceByKey (lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1\n",
    "\n",
    "\n",
    "train_mostFrequent = train_allCounts.sortBy(lambda x: (-x[1], x[0])).take(20000)\n",
    "test_mostFrequent = test_allCounts.sortBy(lambda x: (-x[1], x[0])).take(20000)\n",
    "\n",
    "\n",
    "train_mostFrequentDict = {}\n",
    "for idx, (word, wordCount) in enumerate(train_mostFrequent):\n",
    "   train_mostFrequentDict[word] = idx\n",
    "\n",
    "\n",
    "print(f\"applicant: {train_mostFrequentDict.get('applicant', -1)}\")\n",
    "print(f\"and: {train_mostFrequentDict.get('and', -1)}\")\n",
    "print(f\"attack: {train_mostFrequentDict.get('attack', -1)}\")\n",
    "print(f\"protein: {train_mostFrequentDict.get('protein', -1)}\")\n",
    "print(f\"car: {train_mostFrequentDict.get('car', -1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2\n",
    "\n",
    "\n",
    "def wordsNumpyArray(words, mostFrequentDict):\n",
    "   vector = np.zeros(20000)\n",
    "   for word in words:\n",
    "       if word in mostFrequentDict:\n",
    "           vector[mostFrequentDict[word]] += 1\n",
    "   return vector\n",
    "\n",
    "\n",
    "train_keyAndCountOfWords = train_keyAndListOfWords.map(lambda x: (x[0], wordsNumpyArray(x[1], train_mostFrequentDict)))\n",
    "test_keyAndCountOfWords = test_keyAndListOfWords.map(lambda x: (x[0], wordsNumpyArray(x[1], train_mostFrequentDict)))\n",
    "\n",
    "\n",
    "def find_TF(word_count_vector):\n",
    "   total_num_words = np.sum(word_count_vector)\n",
    "   tf_vector = word_count_vector / total_num_words\n",
    "   return tf_vector\n",
    "\n",
    "\n",
    "def find_IDF(keyAndListOfWords):\n",
    "   word_doc_pairs = keyAndListOfWords.flatMap(lambda x: [(word, x[0]) for word in set(x[1])]).distinct()\n",
    "   total_num_docs = keyAndListOfWords.count()\n",
    "   word_presence = word_doc_pairs.map(lambda x: (x[0], 1))\n",
    "   doc_frequency = word_presence.reduceByKey(lambda a, b: a + b)\n",
    "   idf_vector = doc_frequency.map(lambda x: (x[0], np.log((total_num_docs) / (x[1])))).collectAsMap()\n",
    "   return idf_vector\n",
    "\n",
    "\n",
    "train_idf_vector = find_IDF(train_keyAndListOfWords)\n",
    "test_idf_vector = find_IDF(test_keyAndListOfWords)\n",
    "\n",
    "\n",
    "def find_TFIDF_vector(word_count_vector, idf_vector, mfd):\n",
    "   tf_vector = find_TF(word_count_vector)\n",
    "   tfidf_vector = np.zeros(20000)\n",
    "   for word, index in mfd.items():\n",
    "       if tf_vector[index] > 0:\n",
    "           idf_val = idf_vector.get(word, 0)\n",
    "           tfidf_vector[index] = tf_vector[index] * idf_val \n",
    "   return tfidf_vector\n",
    "\n",
    "\n",
    "train_keyAndTFIDFVector = train_keyAndCountOfWords.map(lambda x: (x[0], find_TFIDF_vector(x[1], train_idf_vector, train_mostFrequentDict)))\n",
    "train_keyAndTFIDFVector.cache()\n",
    "\n",
    "\n",
    "test_keyAndTFIDFVector = test_keyAndCountOfWords.map(lambda x: (x[0], find_TFIDF_vector(x[1], test_idf_vector, train_mostFrequentDict)))\n",
    "test_keyAndTFIDFVector.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARATE DATA AND LABELS AND NORMALIZE DATA\n",
    "\n",
    "\n",
    "def extract_data_and_labels(key_tfidf_pair):\n",
    "   key, tfidf_vector = key_tfidf_pair\n",
    "  \n",
    "   # Extract label based on document id prefix\n",
    "   label = 1 if key[:2] == \"AU\" else 0\n",
    "   return tfidf_vector, label\n",
    "\n",
    "\n",
    "# Apply the mapping function to the RDD\n",
    "train_data_and_labels = train_keyAndTFIDFVector.map(extract_data_and_labels)\n",
    "test_data_and_labels = test_keyAndTFIDFVector.map(extract_data_and_labels)\n",
    "\n",
    "\n",
    "# Split into separate RDDs for data and labels\n",
    "train_data = train_data_and_labels.map(lambda x: x[0])\n",
    "test_data = test_data_and_labels.map(lambda x: x[0])\n",
    "\n",
    "\n",
    "train_vector_sum = np.array(train_data.reduce(lambda x, y: [xi + yi for xi, yi in zip(x, y)]))\n",
    "train_total_count = train_data.count()\n",
    "train_mean_vector = train_vector_sum / train_total_count\n",
    "\n",
    "\n",
    "train_squared_diff_sum = train_data.map(lambda x: np.square(np.array(x) - train_mean_vector)).reduce(lambda x, y: np.add(x, y))\n",
    "train_std_dev_vector = np.sqrt(train_squared_diff_sum / (train_total_count - 1)) * 100\n",
    "\n",
    "\n",
    "nrml_train_data = train_data.map(lambda x: (x - train_mean_vector) / (train_std_dev_vector + 1e-8))\n",
    "nrml_test_data = test_data.map(lambda x: (x - train_mean_vector) / (train_std_dev_vector + 1e-8))\n",
    "\n",
    "\n",
    "train_labels = train_data_and_labels.map(lambda x: x[1])\n",
    "test_labels = test_data_and_labels.map(lambda x: x[1])\n",
    "\n",
    "\n",
    "nrml_train_data.cache()\n",
    "train_labels.cache()\n",
    "nrml_test_data.cache()\n",
    "test_labels.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM GRADIENT DESCENT TRAINING\n",
    "\n",
    "\n",
    "# evaluates the loss function and returns the loss\n",
    "#\n",
    "# x is the data set\n",
    "# y is the labels\n",
    "# w is the current set of weights\n",
    "# c is the weight of the slack variables\n",
    "#\n",
    "def f (x, y, w, lamb):\n",
    "   regularization_term = 0.5 * lamb * np.sum(w ** 2)  # L2 regularization\n",
    "  \n",
    "   loss_term = x.zip(y).map(lambda tfidf_label: -tfidf_label[1] * (tfidf_label[0].dot(w)) + np.log(1 + np.exp(tfidf_label[0].dot(w))))\n",
    "   loss_term_sum = loss_term.reduce(lambda x, y: x + y)\n",
    "\n",
    "\n",
    "   loss = regularization_term + loss_term_sum\n",
    "   return loss\n",
    "\n",
    "\n",
    "# evaluates and returns the gradient\n",
    "#\n",
    "# x is the data set\n",
    "# y is the labels\n",
    "# w is the current set of weights\n",
    "# c is the weight of the slack variables\n",
    "#\n",
    "def gradient(x, y, w, lamb):\n",
    "   gradient = np.zeros(x.first().shape[0]) # x.first().shape[0] is number of dimensions in data set\n",
    "   regularization_term = lamb * w # L2 regularization\n",
    "  \n",
    "   gradient_term = x.zip(y).map(lambda tfidf_label: -tfidf_label[1] * tfidf_label[0] +\n",
    "                                           tfidf_label[0] * (np.exp(tfidf_label[0].dot(w)) /\n",
    "                                           (1 + np.exp(tfidf_label[0].dot(w)))))\n",
    "   gradient_term_sum = gradient_term.reduce(lambda x, y: np.add(x, y))\n",
    "  \n",
    "   gradient += regularization_term + gradient_term_sum\n",
    "  \n",
    "   return gradient \n",
    "               \n",
    "# performs gradient descent optimization, returns the learned set of weights\n",
    "# uses the bold driver to set the learning rate\n",
    "#\n",
    "# x is the data set\n",
    "# y is the labels\n",
    "# w is the current set of weights  to start with\n",
    "# c is the weight of the slack variable\n",
    "#\n",
    "def gd_optimize (x, y, w, lamb):\n",
    "   rate = 0.1\n",
    "   w_last = w + np.full (x.first().shape[0], 1.0)\n",
    "   curr_cost = f(x, y, w, lamb)\n",
    "   prev_cost = f(x, y, w_last, lamb)\n",
    "   for _ in range(100):\n",
    "   # while (abs(curr_cost - prev_cost) > 10e-2):\n",
    "       w_last = w\n",
    "       w = w - rate * gradient (x, y, w, lamb)\n",
    "       curr_cost = f(x, y, w, lamb)\n",
    "       prev_cost = f(x, y, w_last, lamb)\n",
    "       if curr_cost > prev_cost:\n",
    "           rate = rate * .5\n",
    "       else:\n",
    "           rate = rate * 1.1\n",
    "\n",
    "\n",
    "       print (\"cost\", f (x, y, w, lamb))\n",
    "      \n",
    "   return w\n",
    "\n",
    "\n",
    "w = np.zeros (nrml_train_data.first().shape[0])\n",
    "w = gd_optimize (nrml_train_data, train_labels, w, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: MAKE PREDICTIONS\n",
    "\n",
    "\n",
    "# make predictions using all of the data points in x\n",
    "# print ‘success’ or ‘failure’ depending on whether the\n",
    "# prediction is correct\n",
    "#\n",
    "# x is the data set\n",
    "# y is the labels\n",
    "# w is the current set of weights\n",
    "#\n",
    "false_postives = []\n",
    "def predict (x, y, w):\n",
    "   global false_positives\n",
    "   predictions = x.map(lambda point: 1 if point.dot(w) > 0 else 0)\n",
    "   predictions_labels = predictions.zip(y)\n",
    "   tp_count = predictions_labels.filter(lambda pred_label: pred_label[0] == 1 and pred_label[1] == 1).count()\n",
    "   fp_count = predictions_labels.filter(lambda pred_label: pred_label[0] == 1 and pred_label[1] == 0).count()\n",
    "   fn_count = predictions_labels.filter(lambda pred_label: pred_label[0] == 0 and pred_label[1] == 1).count()\n",
    "   tn_count = predictions_labels.filter(lambda pred_label: pred_label[0] == 0 and pred_label[1] == 0).count()\n",
    "   total_predictions = predictions.count()\n",
    "      \n",
    "   correct = tp_count + tn_count\n",
    "   precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "   recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "   f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "  \n",
    "   pred_lbl_idx = predictions_labels.zipWithIndex()\n",
    "   fp_pred_lbl_idx = pred_lbl_idx.filter(lambda pred_lbl_idx: pred_lbl_idx[0][0] == 1 and pred_lbl_idx[0][1] == 0)\n",
    "   fp_idx = fp_pred_lbl_idx.map(lambda pred_lbl_idx: pred_lbl_idx[1])\n",
    "  \n",
    "   false_positives = fp_idx.take(3)\n",
    "  \n",
    "   print(\"tp\", tp_count)\n",
    "   print(\"tn\", tn_count)\n",
    "   print(\"fp\", fp_count)\n",
    "   print(\"fn\", fn_count)\n",
    "  \n",
    "   print(f\"{correct} out of {total_predictions} correct.\")\n",
    "   print(f\"Precision: {precision}\")\n",
    "   print(f\"Recall: {recall}\")\n",
    "   print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "\n",
    "predict (nrml_test_data, test_labels, w)\n",
    "print(\"I tried but I couldn't figure out what the bug in my code was :(\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GET TOP 50 WORDS WITH CORRESPONDING REGRESSION COEFFICIENT\n",
    "\n",
    "\n",
    "w_copy = w.copy()\n",
    "w_list = list(w_copy)\n",
    "sorted_w = np.sort(w_copy)[::-1]\n",
    "indexes = []\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "   indexes.append(w_list.index(sorted_w[i]))\n",
    "  \n",
    "reverse_mfd = {}\n",
    "for key, val in train_mostFrequentDict.items():\n",
    "   reverse_mfd[val] = key\n",
    "\n",
    "\n",
    "print(\"Top 50 words with largest regression coefficients\")\n",
    "for i in range(len(indexes)):\n",
    "   print(f\"Number {i + 1} word: {reverse_mfd[indexes[i]]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
